# Causal-chat
*只需上传你的数据集，Causal-chat 可以自动帮你挑选并得到因果分析算法，几秒中内生成可交互的对话面板 + 专业报告。*

## 项目优势：
- 因果报告分析：随着大语言模型的流行，社会上的一些因果性被更多人所知。但是因果推断并不是一个通俗易懂的框架。本项目以大预言模型为基础框架，以聊天的形式，简化分析流程，给用户一个详尽且专业的报告。
- 可交互：当用户遇到不明白的因果问题，可以选择继续追问，让用户更能理解因果推断的问题
- 独立的交互平台：重新构建的chat的应用，为未来拓展功能做框架。
- 全新架构：causalchat运用MCP全新架构，实现可拆卸，可更换的新一代框架，做到拓展性高，成本投入少等优点。
- 现代化的前后端构架：本项目依FLASK现代应用组件，实现网页段和应用端的双重实现，实现前后端分离。
  

## 依赖安装
本项目使用 Python，所有依赖项都已在 `requirements.txt` 文件中列出。请使用以下命令安装：
```bash
pip install -r requirements.txt
```
*注意: `gunicorn` 仅适用于 Linux/macOS。*

## 开发流程 (Windows/macOS/Linux)

这是一个前后端分离的项目，后端由 Flask 提供服务，前端由 `pywebview` 构建为一个独立的桌面应用窗口。

### 1. 启动后端服务
在项目根目录 (`causalchat/`) 下打开一个终端，运行以下命令：
```bash
python Causalchat.py
```
你会看到 Flask 开发服务器启动的日志，它正在 `http://127.0.0.1:5000` 上监听。**请保持此终端窗口运行。**

### 2. 启动前端应用
再打开一个 **新的终端窗口**，同样在项目根目录下，运行以下命令：
```bash
python run_webview.py
```
稍等片刻，一个标题为 "FLYINGPIG-AI" 的桌面应用窗口将会出现，并加载应用的登录界面。

## 生产部署流程

在生产环境中，后端将部署在云服务器上，而前端应用 (`Run_causal.py`) 在本地电脑上运行并连接到服务器。

### 1.1 服务器端 (Linux)
- 将项目代码同步到你的云服务器。
- 在服务器的 `causalchat/` 目录下，使用 `gunicorn` 启动后端服务：
  ```bash
  gunicorn Causalchat:app -w 4 -b 0.0.0.0:5000
  ```
- 确保服务器的防火墙或安全组已开放 `5000` 端口。
### 1.2 服务器端（windows）
运行Causalchat.py

### 2. 客户端 (你的本地电脑)
- 在你的本地电脑上，用代码编辑器打开 `causalchat/Run_causal.py` 文件。
- 修改文件顶部的 `URL` 变量，将其指向你的云服务器的公网 IP 地址。
  ```python
  # 例如:
  URL = 'http://120.46.198.198:5001' 
  ```
- 保存文件后，在本地终端中运行前端应用：
  ```bash
  python Run_causal.py
  ```
- 桌面应用窗口将会启动，并直接连接到你在云服务器上的后端服务。


## 更新日志：


---
2025.5.9
1. 完成LLM chat框架的全构建
2. 完成数据库的统一化，更加安全和规范
3. 增加上传csv文件功能  
4. 增加后端检查上传文件功能

---
2025.5.10
1. 构建mysql数据库
2. 前将后端部署到服务器
3. 修改了前后端交互的方式，引入gunicorn进行交互

---
2025.5.11
1. gunicorn部署成功，支持多用同时登录的并行检测
2. 进行flask加密，实现多用户登录的密匙检测，去除config的本地检测
3. 修改了mysql数据库的错误实现

---
2025.6.11
1. 实现MCP的小demo
2. 在回答函数部分，我们构建了异步工作的实现逻辑
   
---
2025.6.12
1. 实现因果库，可以进行简单的因果pc算法分析
2. 实现mcp连通因果库，通过mcp的检测，先可以让llm主动选择调用还是不调用库
3. 实现调用vis-network库，将图表数据（节点和边）在这个区域内渲染成一个功能齐全、可供用户交互的动态图表。
4. 增加llm上下文阅读功能，现在LLm已经有记忆了，目前最高支持20条历史记录
5. 增加了代码中创建了一个在后台线程中持续运行的 asyncio 事件循环。
6. 新增历史记录支持保存因果图功能
7. 增加应用端多用户并行登录逻辑
8. 增加前端动画效果，加载动画
---